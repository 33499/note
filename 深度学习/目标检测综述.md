​      

### 一、 摘要

​      目标检测是很多计算机视觉任务的基础，不论我们需要实现图像与文字的交互还是需要识别精细类别，它都提供了可靠的信息。本文对目标检测进行了整体回顾，第一部分从 RCNN 开始介绍基于候选区域的目标检测，包括 Fast R-CNN、Faster R-CNN 和 R-FCN 等；第二部分则重点讨论了包括 YOLO、SSD 和 RetinaNet 等在内的单次检测器，它们都是目前最为优秀的方法。目标检测目前有 one-stage 和 two-stage 两种，two-stage 指的是检测算法需要分两步完成，首先需要获取候选区域，然后进行分类，比如R-CNN系列；与之相对的是 one-stage 检测，可以理解为一步到位，不需要单独寻找候选区域，典型的有SSD/YOLO。第三部分研究目标检测最新发表的论文成果，对这些最新的方法做出剖析，找到其文章的精髓以及在目标检测方面仍需改进的地方，分析这些论文的异同点。

### 二、基于候选区域的目标检测

##### 2.1 滑动窗口检测器

自从 AlexNet 获得 ILSVRC 2012 挑战赛冠军后，用 CNN 进行分类成为主流。一种用于目标检测的暴力方法是从左到右、从上到下滑动窗口，利用分类识别目标。为了在不同观察距离处检测不同的目标类型，需要使用不同大小和宽高比的窗口。

![img](https:////upload-images.jianshu.io/upload_images/15050749-e9db76a36c5e7515.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

滑动窗口（从左到右，从上到下）

我们根据滑动窗口从图像中剪切图像块。由于很多分类器只取固定大小的图像，因此这些图像块是经过变形转换的。但是，这不影响分类准确率，因为分类器可以处理变形后的图像。

![img](https:////upload-images.jianshu.io/upload_images/15050749-5b21011feae1b168.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/310/format/webp)

将图像变形转换成固定大小的图像

变形图像块被输入 CNN 分类器中，提取出 4096 个特征。之后，我们使用 SVM 分类器识别类别和该边界框的另一个线性回归器。

![img](https:////upload-images.jianshu.io/upload_images/15050749-779b2c32a92a5572.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

滑动窗口检测器的系统工作流程图

下面是伪代码。我们创建很多窗口来检测不同位置的不同目标，要提升性能，一个显而易见的办法就是减少窗口数量。



```dart
for window in windows:
    patchs = get_patch(image, window)
    results = detector(patchs)
```

##### 2.2 R-CNN

实际上，更实用的方法是**候选区域**（Region Proposals ）方法来获取感兴趣的区域（ROI）。**选择性搜索**（Selective Search ）就是一种典型的候选区域方法。算法原理如下：首先将每个像素作为一组。然后，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的 ROI。与寻找几乎个区域比起来，这种方法要高效的多。

![img](https:////upload-images.jianshu.io/upload_images/15050749-efc1e491307e5d25.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

选择性搜索算法过程

R-CNN 利用候选区域方法创建了约 2000 个 ROI。这些区域被转换为固定大小的图像，并分别送到卷积神经网络中。之后使用SVM对区域进行分类，使用线性回归损失来校正边界框，以实现目标分类并得到边界框。以下是 R-CNN 整个系统的流程图：

![img](https:////upload-images.jianshu.io/upload_images/15050749-dd8e918b8b81520c.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

通过使用更少且更高质量的 ROI，R-CNN 要比滑动窗口方法更快速、更准确。伪代码如下：



```undefined
ROIs = region_proposal(image)
for ROI in ROIs
    patch = get_patch(image, ROI)
    results = detector(patch)
```

##### 2.3 Fast R-CNN

R-CNN 需要非常多的候选区域以提升准确度，但其实有很多区域是彼此重叠的。如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们可能需要重复提取很多次特征。因此 R-CNN 的训练和预测速度非常慢。

此外，CNN 中的特征图以一种密集的方式表征空间特征，那么我们能直接使用特征图代替原图来检测目标吗？答案是肯定的。

![img](https:////upload-images.jianshu.io/upload_images/15050749-98e33de255bd0aa7.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

![img](https:////upload-images.jianshu.io/upload_images/15050749-a5b72154def85bd2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

直接利用特征图计算 ROI

Fast R-CNN 使用CNN网络先提取整个图像的特征，而不是对每个图像块提取多次。然后，我们可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 来生成 ROI区域在对应的特征图上的映射特征图块，并用于目标检测任务中。我们使用 ROI 池化将特征图块转换为固定的大小，并送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，所以它能显著地减少处理时间。

![img](https:////upload-images.jianshu.io/upload_images/15050749-9baef83956074f31.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

将候选区域直接应用于特征图，并使用 ROI 池化将其转化为固定大小的特征图块

以下是 Fast R-CNN 的流程图：

![img](https:////upload-images.jianshu.io/upload_images/15050749-6ab7846d5726b7fa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

**ROI 池化**
 因为 Fast R-CNN 使用全连接层，所以我们应用 ROI 池化将不同大小的 ROI 转换为固定大小。比如我们将 8×8 特征图转换为预定义的 2×2 大小：

![img](https:////upload-images.jianshu.io/upload_images/15050749-67aa219b1dd1ae27.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/735/format/webp)

输入特征图（左上），输出特征图（右下），ROI (右上，蓝色框)

左上角：输入特征图；
 右上角：将 ROI（蓝色区域）与特征图重叠；
 左下角：将 ROI 拆分为目标维度。例如，对于 2×2 目标，我们将 ROI 分割为 4 个大小相似或相等的部分；
 右下角：找到每个部分的最大值，得到变换后的特征图。

按上述步骤得到一个 2×2 的特征图块，可以送至分类器和边界框回归器中。使用softmax损失进行分类；使用回归损失比如平滑L1损失校正包围框。总损失是两部分的和，然后反向传播进行训练。Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。

在下面的伪代码中，计算量巨大的特征提取过程从 For 循环中移出来了，因此速度得到显著提升。Fast R-CNN 的训练速度是 R-CNN 的 10 倍，预测速度是后者的 150 倍。



```undefined
feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
```

##### 2.4 Faster R-CNN

Fast R-CNN 依赖于外部候选区域方法，如选择性搜索。但这些算法在 CPU 上运行且速度很慢。在测试中，Fast R-CNN 需要 2.3 秒来进行预测，其中 2 秒用于生成 2000 个 ROI。因此区域生成的计算成为整个检测网络的瓶颈。

与其使用固定的算法得到候选区域，不如让网络自己学习自己的候选区域应该是什么。因此，Faster R-CNN 采用与 Fast R-CNN 相同的设计，只是它用区域生成网络（Region Proposal Network，**RPN**）代替了候选区域方法。新的候选区域网络（RPN）在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。

![img](https:////upload-images.jianshu.io/upload_images/15050749-7010a624bd92a181.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

Faster R-CNN 的流程图与 Fast R-CNN 基本相同，只是使用RPN代替了原来的外部候选区域方法

**RPN**
 区域生成网络（RPN）将前面卷积网络的输出特征图作为输入，比如VGG16的conv5特征图。它在特征图上滑动一个 3×3 的卷积核，以使用卷积网络构建与类别无关的候选区域。使用VGG网络提取特征的话，每个 3x3 区域会得到一个512维的特征向量，然后送到两个独立的全连接层，以预测边界框和两个目标分数（是目标或者不是目标）。我们其实可以使用回归器计算单个 目标分数，但为简洁起见，Faster R-CNN 使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。

![img](https:////upload-images.jianshu.io/upload_images/15050749-778f0de02000f8aa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1008/format/webp)

对于特征图中的每一个位置，RPN 会做 k 次预测。因此，RPN 将输出 4×k 个坐标和每个位置上 2×k 个得分。下图展示了 8×8 的特征图，且有一个 3×3 的卷积核执行运算，它最后输出 8×8×3 个 ROI（其中 k=3）。下图（右）展示了单个位置的 3 个候选区域。

![img](https:////upload-images.jianshu.io/upload_images/15050749-e89a8b6f31428a4c.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/688/format/webp)

上图每个位置有 3 种猜想，稍后我们将予以完善。由于只需要一个正确猜想，因此我们最初的猜想最好涵盖不同的形状和大小。因此，Faster R-CNN 不会创建随机边界框。相反，它会预测一些与左上角名为**锚点**的参考框相关的偏移量（如𝛿x、𝛿y）。要对每个位置进行 k 个预测，我们需要以每个位置为中心的 k 个锚点。这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以以更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。

![img](https:////upload-images.jianshu.io/upload_images/15050749-d541798e76523062.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/350/format/webp)

Faster R-CNN 使用更多的锚点。它部署 9 个锚点框：3 个不同宽高比的 3 个不同大小的锚点框。每一个位置使用 9 个锚点，每个位置会生成 2×9 个目标分数和 4×9 个坐标。

![img](https:////upload-images.jianshu.io/upload_images/15050749-525f25f7360776de.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

伪代码：



```rust
feature_maps = process(image)
ROIs = rpn(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    class_scores, box = detector(patch) 
    class_probabilities = softmax(class_scores)
```

这部分详细的内容可以参考：[Faster R-CNN：使用RPN实时目标检测](https://www.jianshu.com/p/643cdcf674fc)

##### 2.5 F-RCN

R-FCN 通过减少每个 ROI 所需的工作量实现加速。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。



```bash
feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = region_roi_pool(score_maps, ROI)     
    class_scores, box = average(V)                   # Much simpler!
    class_probabilities = softmax(class_scores)
```

### 三、One-Stage 目标检测

这部分我们将对单次目标检测器（包括 SSD、YOLO、YOLOv2、YOLOv3）进行综述。我们将分析 FPN 以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析 Focal loss 和 RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。

**one-stage检测器**

Faster R-CNN 中有一个专用的候选区域网络RPN。基于区域的检测器是很准确的，但需要付出代价。Faster R-CNN 在 PASCAL VOC 2007 测试集上每秒处理 7 帧的图像（7 FPS）。作为替代，我们是否需要一个分离的候选区域步骤？我们可以直接在一个步骤内得到边界框和类别吗？



```bash
feature_maps = process(image)
results = detector3(feature_maps) # No more separate step for ROIs
```

让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当做初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。

![img](https:////upload-images.jianshu.io/upload_images/15050749-ea3dc19e0dcb312d.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

基于滑动窗口进行预测

这个概念和 Faster R-CNN 中的锚点很相似。然而，单次检测器会同时预测边界框和类别。例如，我们有一个 8 × 8 特征图，在每个位置，我们有 k 个锚点（锚点是固定的初始边界框猜想），一个锚点对应一个特定位置。我们使用相同的锚点形状仔细地选择锚点和每个位置，并在每个位置做出 k 个预测，即总共有 8 × 8 × k 个预测结果。

![img](https:////upload-images.jianshu.io/upload_images/15050749-8b38e7f742543ebf.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/335/format/webp)

使用 3 个锚点在每个位置做出 3 个预测

以下是 4 个锚点（绿色）和 4 个对应预测（蓝色），每个预测对应一个特定锚点。

![img](https:////upload-images.jianshu.io/upload_images/15050749-4638a58f8d2aeb31.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

4 个预测，每个预测对应一个锚点

在 Faster R-CNN 中，我们使用卷积核来做 6个参数的预测：4 个参数对应某个锚点的预测边框，2个参数对应 objectness 置信度得分。因此 3× 3× D × 6 卷积核将特征图从 8 × 8 × D 转换为 8 × 8 × 6。

![img](https:////upload-images.jianshu.io/upload_images/15050749-fd715e3a3b7d59ea.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/708/format/webp)

使用 3x3 卷积核计算预测

在单次检测器中，卷积核还预测 C 个类别概率以执行分类（每个概率对应一个类别，包含背景）。因此我们应用一个 3× 3× D × 25 卷积核将特征图从 8 × 8 × D 转换为 8 × 8 × 25 （C=20，（dx, dy, dh, dw, confidence）4个边框参数，1个置信度）。

![img](https:////upload-images.jianshu.io/upload_images/15050749-255accd9f10a27a8.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1047/format/webp)

每个位置做出 k 个预测，每个预测有 25 个参数

单次检测器通常需要在准确率和实时处理速度之间进行权衡。它们在检测太近距离或太小的目标时容易出现问题。在下图中，左下角有 9 个圣诞老人，但某个单次检测器只检测出了 5 个。

![img](https:////upload-images.jianshu.io/upload_images/15050749-21f689efb9867433.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

##### 3.1 SSD

SSD（Single-Shot MultiBox Detector），使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。

![img](https:////upload-images.jianshu.io/upload_images/15050749-aa15690a663266b4.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

同时对类别和位置执行单次预测

然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。

![img](https:////upload-images.jianshu.io/upload_images/15050749-a178bfd6934f49c1.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

使用多尺度特征图用于检测

以下是特征图图示。

![img](https:////upload-images.jianshu.io/upload_images/15050749-2552e0c902046afa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

image

SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。

![img](https:////upload-images.jianshu.io/upload_images/15050749-07e817f1ff6e3c31.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

##### 3.2 YOLO / v2 / v3

YOLO （You Only Look Once）是另一种单次目标检测器。YOLO 在卷积层之后使用了 DarkNet 来做特征检测。

![img](https:////upload-images.jianshu.io/upload_images/15050749-6a8b08870bac9b66.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

然而，它并没有使用多尺度特征图来做独立的检测。相反，它将特征图部分平滑化，并将其和另一个较低分辨率的特征图拼接。例如，YOLO 将一个 28 × 28 × 512 的层重塑为 14 × 14 × 2048，然后将它和 14 × 14 ×1024 的特征图拼接。之后，YOLO 在新的 14 × 14 × 3072 层上应用卷积核进行预测。

YOLO v2做出了很多实现上的改进，比如使用BN层、使用高分辨率图像微调分类模型、采用Anchor Boxes、约束预测边框的位置等，将 mAP 值从第一次发布时的 63.4 提高到了 78.6。YOLO9000 可以检测 9000 种不同类别的目标。

YOLO v3主要的改进有：调整了网络结构；利用多尺度特征进行对象检测；对象分类用logistic取代了softmax。**下面主要介绍一下YOLO v3。**（参考文章：[YOLO v3深入理解](https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F49556105)）

**网络架构**

在基本的图像特征提取方面，YOLO v3采用了称之为**Darknet-53**的网络结构（含有52个卷积层1个全连接层），它借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路（shortcut connections）。DarkNet-53 主要由 3 × 3 和 1× 1 的卷积核以及类似 ResNet 中的快捷链路构成。相比 ResNet-152，DarkNet 有更低的 BFLOP（十亿次浮点数运算），但能以 2 倍的速度得到相同的分类准确率。

![img](https:////upload-images.jianshu.io/upload_images/15050749-defc4c9bac5b4a26.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/334/format/webp)

上图的Darknet-53网络采用256*256*3作为输入，最左侧那一列的1、2、8等数字表示多少个重复的残差组件。每个残差组件有两个卷积层和一个快捷链路，示意图如下：

![img](https:////upload-images.jianshu.io/upload_images/2709767-bb96f29a37be1651.png?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)

残差组件

**特征金字塔网络（FPN）——利用多尺度特征进行检测**

YOLO v2曾采用passthrough结构来检测细粒度特征，在YOLO v3更进一步采用了3个不同尺度的特征图来进行对象检测。

![img](https:////upload-images.jianshu.io/upload_images/15050749-ad6fa6591d044091.png?imageMogr2/auto-orient/strip|imageView2/2/w/1000/format/webp)

结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416x416的话，这里的特征图就是13x13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。

为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。

最后，第91层特征图再次上采样，并与第36层特征图融合，最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。

上面使用不同尺寸特征图进行预测的网络称为特征金字塔网络（FPN），是一种旨在提高准确率和速度的特征提取器。数据流如下：

![img](https:////upload-images.jianshu.io/upload_images/15050749-64bf69f7278315d4.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

FPN

FPN 由自下而上和自上而下路径组成。其中自下而上的路径是用于特征提取的常用卷积网络。空间分辨率自下而上地下降。当检测到更高层的结构，每层的语义值增加。

![img](https:////upload-images.jianshu.io/upload_images/15050749-03de5ed9a022f2f0.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/470/format/webp)

FPN中的特征提取

SSD 通过多个特征图完成检测。但是，最底层不会被选择执行目标检测。它们的分辨率高但是语义值不够，导致速度显著下降而不能被使用。SSD 只使用较上层执行目标检测，因此对于小的物体的检测性能较差。

![img](https:////upload-images.jianshu.io/upload_images/15050749-5b22f5a8a1e50a95.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)

FPN 提供了一条自上而下的路径，从语义丰富的层利用上采样构建高分辨率的层。

![img](https:////upload-images.jianshu.io/upload_images/15050749-05a2fffcb407e080.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

自上而下重建空间分辨率

虽然该重建层的语义较强，但在经过所有的上采样和下采样之后，目标的位置不精确。在重建层和相应的特征图之间添加横向连接可以使位置侦测更加准确。

![img](https:////upload-images.jianshu.io/upload_images/15050749-ccf23bd8f5bc749f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

增加跳过连接

FPN 可以结合 Fast R-CNN 或 Faster R-CNN。在 FPN 中，我们生成了一个特征图的金字塔，然后用 RPN（来生成 ROI。基于 ROI 的大小，我们选择最合适尺寸的特征图层来提取特征块。

![img](https:////upload-images.jianshu.io/upload_images/15050749-5dc2dd452f9b8722.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1000/format/webp)

**9种尺度的 anchor boxes**

随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。

分配上，在最小的13x13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26x26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52x52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。

![img](https:////upload-images.jianshu.io/upload_images/2709767-5cc00a60004e0473.png?imageMogr2/auto-orient/strip|imageView2/2/w/992/format/webp)

感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。

![img](https:////upload-images.jianshu.io/upload_images/2709767-f7e39ba644a583e4.png?imageMogr2/auto-orient/strip|imageView2/2/w/632/format/webp)

**分类softmax改成logistic**

预测对象类别时不使用softmax，改成使用logistic的输出进行预测。

**输入映射到输出**

![img](https:////upload-images.jianshu.io/upload_images/2709767-9b28d0f1c682b80a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

不考虑神经网络结构细节的话，总的来说，对于一个输入图像，YOLO v3将其映射到3个尺度的输出张量，代表图像各个位置存在各种对象的概率。

我们看一下YOLO v3共进行了多少个预测。对于一个416x416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13x13x3 + 26x26x3 + 52x52x3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。

对比一下，YOLO v2采用13x13x5 = 845个预测，YOLO v3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。

##### 3.3 RetianNet

RetianNet是基于 ResNet、FPN以及利用 Focal loss 构建的 。

![img](https:////upload-images.jianshu.io/upload_images/15050749-a8cea15ead3d5a99.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

RetinaNet

**Focal Loss**

类别不平衡会损害性能。SSD 在训练期间重新采样目标类和背景类的比率，这样它就不会被图像背景淹没。Focal loss（FL）采用另一种方法来减少训练良好的类的损失。因此，只要该模型能够很好地检测背景，就可以减少其损失并重新增强对目标类的训练。我们从交叉熵损失 （Cross Entroy Loss）开始，并添加一个权重来降低高可信度类的交叉熵。

![img](https:////upload-images.jianshu.io/upload_images/15050749-324f6d6caf3144fa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/800/format/webp)

例如，令 γ = 0.5, 经良好分类的样本的 Focal Loss 趋近于 0。

![img](https:////upload-images.jianshu.io/upload_images/15050749-fbb52eafebcded04.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)



### 四、2020年目标检测最新成果

##### 4.1 BiDet: An Efficient Binarized Object Detector

**Motivation**：常规的网络二值化方法直接对具有受限表示能力的一阶段或二阶段检测器中的权重和激活进行量化，网络中的信息冗余会导致大量误报并严重降低性能。

**Method**：文章提出了一种称为BiDet的二值化神经网络学习方法，充分利用二进制神经网络的表示能力，通过冗余去除来进行目标检测，从而通过减少误报来提高检测精度。性能比现有的二值化神经网络要好得多。

更具体地说，我们将信息瓶颈（IB）原则强加给二值化对象检测器学习，其中我们同时限制了高级特征图中的信息量，并最大化了对象检测与学习的特征图之间的互信息。同时，在IB中利用学习到的稀疏对象先验，从而迫使后验者集中于信息预测，并消除了无用的误报。

**contributions**：1.提出了第一个包含骨干网和检测部分的二值化网络，以进行有效的对象检测。2.我们采用IB原则进行冗余去除，以充分利用二进制神经网络的能力，并学习稀疏对象先验以将后代集中在信息检测预测上，从而通过假阳性消除来提高检测精度。3.与最新的二进制神经网络进行对象检测的全面比较。

---

##### 4.2 EfficientDet: Scalable and Efficient Object Detection

**Motivation**:  多尺度特征融合和模型规模问题:**由于这些不同的输入特征的分辨率不同，因此我们观察到它们通常对融合输出特征的贡献不均**; **先前的工作主要依靠更大的骨干网或更大的输入图像大小来获得更高的准确性**

**Method**:  1.提出bifpn，它引入了可学习的权重，以了解不同输入特征的重要性，同时反复应用自上而下和自下而上的多尺度特征融合；2.缩放特征网络也很重要，提出了一种用于对象检测器的复合缩放方法，该方法可联合扩展所有骨干网，特征网络，盒子/类预测网络的分辨率/深度/宽度。**将EfficientNet主干网与BiFPN和复合缩放相结合提出EfficientDet，它以更少的参数和浮点数获得更高的精度。**

---

##### 4.3 AugFPN: Improving Multi-scale Feature Learning for Object Detection

**Motivation**：找到FPN设计上的一些缺陷，主要有三个。1.不同级别的要素之间的语义差距。2.高级别特征图的信息丢失。3.ROIs的启发式分配策略。最大融合会忽略响应较小的特征，这可能也有帮助，但仍无法充分利用其他级别的特征。

**Method** : 1.提出了一致性监督，通过在这些特征图上施加相同的监督信号，使横向连接后的特征图包含相似的语义信息。2.利用比率不变的自适应池来提取各种上下文信息，从而可以减少残差特征金字塔中高层特征的信息丢失,称为残差特征增强。3.第三，引入了软RoI选择，以更好地利用不同金字塔等级的RoI功能，并为后续的位置细化和分类产生更好的RoI功能

**contributions**：找出了FPN设计的三个缺陷；提出AugFPN解决以上三个问题，AugFPN包括Consistent Supervision, Residual Feature Augmentation, and Soft RoI Selection ；对MSCOCO上配备了各种检测器和主干网络的AugFPN进行了评估，与基于检测器的FPN相比，它始终带来重大改进.

---

##### 4.4 Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection

**Motivation**:  由于FPN 和Focal Loss 的出现，最近的学术注意力转向了无锚框检测器。这篇研究了基于锚的方法与免锚的方法之间的差异。基于锚点的检测与不基于锚点的检测之间的本质区别实际上是如何定义正训练样本和负训练样本，这导致它们之间的性能差距。

**Method**:  提出了一种自适应训练样本选择（ATSS），可以根据对象的统计特征自动选择正样本和负样本。

**Contributionzhebpian**

* 指出基于锚和无锚检测器之间的本质区别实际上是如何定义正训练样本和负训练样本
* 提出自适应训练样本选择，以根据对象的统计特征自动选择正负训练样本
* 证明在图像上的每个位置平铺多个锚点以检测对象是无用的
* 在不引入任何额外开销的情况下，在MS COCO上实现最先进的表现。

---

##### 4.5 Dynamic Refinement Network for Oriented and Densely Packed Object Detection

**Motivation**: **定向和密集的目标检测仍然存在问题**

​                        1. 神经元的感受野都是相同形状呈现轴向排列，而目标通常形状不同，并且沿着不同方向排列

​                        2. 检测模型一般使用通用知识进行训练，在测试阶段时可能不能很好地泛化处理特定的目标；

​                        3. 有限的数据集阻碍了这类任务的发展。

**Method**: 为解决第1个和第2个问题，作者提出了动态精细化网络，包括FSM（特征选择模块）、DRH（动态优化 head）。——FSM能够根据目标物体的形状和方向位置调整神经元的感受野。DRH能使模块以目标感知的方式动态地优化预测。

**Contribution**: 1. 提出一种**基于目标形状和方向的自适应调整神经元感受野**的模块。该FSM模块能有效地缓解感受野和目标之间的失调.2提出两个DRH，即**DRH-C和DRH-R**（分别用于分类和回归任务）。这些DRHs可以根据每一个样本的唯一性和特殊性建模，并以一种面向对象的方法进行预测.3.收集了SKU110K-R，包含对定向边界框精细标注.4.在DOTA、HRSC2016、SKU110K和SKU110KR中，我们的方法在定向和密集填充对象检测方面取得了一致和实质性的进展

---

##### 4.6 Harmonizing Transferability and Discriminability for Adapting Object Detectors

**Abstract**

* 凭借对抗性特征自适应(adversarial feature adaptation)来减轻检测管道的分布偏移，自适应目标检测(adaptive object detection)取得了令人瞩目的结果

* 鉴于对象的复杂组合以及域之间差异化的场景布局，可传递性和可辨别性在对抗性适应中可能会出现矛盾。

* 在本文中，我们提出了一种分层可传递性校准网络（HTCN），该网络可分层（局部区域/图像/实例）校准特征表示的可传递性，以协调可传递性和可辨别性

* 提出的模型包含三个模块：1.具有输入插值（IWAT-I）的重要性加权对抗训练，通过对插值的图像级特征重新加权来增强全局可分辨性   2.上下文感知的实例级对齐（CILA）模块，它通过捕获实例级功能和全局上下文信息之间的基础互补效应（实例级功能对齐）来增强本地可分辨性；3.局部特征蒙版，用于校准局部可传递性，从而为随后的区别性模式对齐提供语义指导。实验结果表明，HTCN明显优于基准数据集上的最新方法。

  

**Introduction**

* 由于存在域移位，在源域上训练的对象检测器不能很好地推广到新的目标域.通过将知识从标记的源域转移到完全未标记的目标域，无监督域自适应（UDA)是解决该问题的有前途的解决方案。
* UDA的一般做法是通过显式学习域之间的不变表示形式并在源域上实现小错误来弥合域鸿沟，这在图像分类方面取得了令人信服的性能.这些UDA方法可以分为两大类:第一类是统计匹配，其目的是通过统计分布差异来匹配跨域的特征;第二类是对抗学习，旨在通过领域对抗训练或基于GAN的像素级自适应来学习领域不变表示。
* 由于检测任务的局部性，当前的方法通常通过对抗性特征自适应，例如图像和实例水平对齐，强局部和弱全局对齐，局部区域对齐，将多级域的差异最小化,基于区域建议，具有预测指导的实例级约束的多级特征对齐。它们的共同点是利用对抗性适应能力可以产生明显的可转移性
* 因为不是所有特征都可以平等转移，对抗性适应可能会损害目标特征的可分辨性。在本文中，可传递性是指跨域的学习表示的不变性，可分辨性是指检测器定位和区分不同实例的能力。考虑到各种对象的复杂组合以及域之间差异化的场景布局，这种现象在跨域检测中将更加严重。换句话说，通过对抗性学习来严格对齐域之间的整个特征分布很容易导致负迁移，因为在对象检测器中没有明确阐述不同级别（即局部区域，实例和图像）的可迁移性
* 提出了Hierarchical Transferability Calibration Network (HTCN)，通过分级校准表示的可传递性和可分辨性来规范对抗性适应。具体来说，首先提出一种具有输入插值的重要性加权对抗训练（IWAT-I）策略，该策略旨在通过基于并非所有样本都可以同等转移的动机（尤其是在插值之后）重新加权插值特征空间来增强全局可辨性。
* 考虑到结构化的场景布局和检测任务的局部性质，我们设计了上下文感知的实例级对齐（CILA）模块，以通过捕获实例级功能和全局上下文之间的互补效应来增强局部可分辨性。特别是，我们的方法不是简单地将这两个术语连接在一起，而是采用张量积进行更多的信息融合。
* 最后，在观察到整个图像的某些局部区域比其他局部区域更具描述性和优势后，我们建议通过基于浅层特征计算两个域中的局部特征掩码以近似地指导语义一致性，从而进一步增强局部可辨性。
  对齐之后，可以看作是一个类似注意的模块，以无监督的方式捕获可转移区域。

**contribution**： 提出的HTCN通过协调可传递性和可辨别性之间的潜在矛盾，大大扩展了以前基于对抗的自适应检测方法的能力。大量实验表明，该方法在跨域检测的多个基准数据集上已超过了最新技术。

##### 4.7 Detection in Crowded Scenes: One Proposal, Multiple Predictions

**Abstract**

* 我们提出了一个简单但有效的基于提议的对象检测器，旨在***检测拥挤场景中的高度重叠实例***，我们方法的关键是让每个提案预测一组相关实例，而不是先前基于提案的框架中的单个实例。
* 配备了EMD损失和Set NMS等新技术，我们的探测器可以有效地处理探测高度重叠的物体的困难。

**Introduction**

* 基于提案的框架已广泛用于现代对象检测系统,包括one-stage,two-stage,multi-stage.这些方法通常有两个步骤：首先，以手工方式（例如预定义的锚点[28、24、21]）或可学习的方式（例如RPNs [29、20、13]）生成完整的对象预框；然后，通过置信度分数和精确的位置预测与每个投标框相对应的单个实例。
* 尽管基于预选框的方法获得了最先进的性能，**但是在实践中，对于拥挤的检测来说仍然非常具有挑战性。检测器无法预测与其他对象严重重叠的实例**
* 这种在**拥挤场景中的典型失败主要归因于两个原因**，1：首先，高度重叠的实例（及其相关提案）可能具有非常相似的特征。检测器难以分别为每个提议生成区别预测 2：由于实例可能彼此严重重叠，预测很可能会被NMS错误地压制。

* **先前的解决方法：以前的工作试图从不同的角度解决此问题，例如复杂的NMS [1、16、23、18、25、17]，new loss functions[44、40]，re-scoring[19]，part-based detectors[44，38，46，3]。但是，当前的工作对于处理高度重叠的例子或过于复杂或效率较低，或降低了较少重叠的案件的性能。**
* **论文提出的方法：对于每个预选框，建议不要像往常那样预测单个实例，而建议预测一组可能高度重叠的实例。通过这种方案，预期附近提议的预测可以推断出相同的实例集，而不是区分个体，这很容易学习。另外，首先，还提出了EMD损失来监督实例集预测的学习。其次，引入了一种新的名为Set NMS的后处理方法，以抑制来自不同提议的重复项，旨在克服人群场景中单纯NMS的缺点。最后，一个可选的优化模块（RM）设计用于处理潜在的误报**

---

##### 4.8 Learning a Unified Sample Weighting Network for Object Detection

**Abstract**

* 区域采样或加权对于现代基于区域的对象检测器的成功至关重要。

* 与之前的一些工作（在优化目标函数时只关注“硬”样本）不同，我们认为样本权重应与数据有关且与任务有关.样本对于目标函数优化的重要性取决于对象分类和边界框回归任务的不确定性。

* 为此，我们设计了一个通用损失函数，以各种采样策略覆盖大多数基于区域的物体检测器，然后在此基础上，我们提出了一个统一的样本加权网络来预测样本的任务权重

* 其方法是简单有效的，有如下优点：

  **1.它可以共同学习分类和回归任务的样本权重，这使其与大多数以前的工作有所不同。**

  **2.这是一个数据驱动的过程，因此避免了一些手动参数调整**

  **3.它可以轻松插入大多数物体检测器中，并在不影响其推理时间的情况下实现了显着的性能提升**

**Introduction**

* 现代的基于区域的对象检测是一个多任务学习问题，由对象分类和定位组成。它涉及区域采样（滑动窗口或区域建议），区域分类和回归以及非最大抑制。根据区域搜索的方式，这些检测器可以分为one stage[29、31、35、46]和two stage[2、15、16、17、28、37]。

* 通常，精度最高的对象检测器基于两阶段框架，例如Faster RCNN [37]，该框架在区域提议阶段会迅速缩小区域范围（大部分来自背景）。相比之下，单级检测器（例如SSD [31]和YOLO [35]）实现了更快的检测速度，但准确性较低。**这是由于类别不平衡问题（即，前景和背景区域之间的不平衡），这是对象检测的经典挑战。**

* two-stage检测器通过区域提议机制处理类别不平衡，然后采用各种有效的采样策略，例如以**固定的前景与背景之比进行采样**以及进行**hard example mining(困难样本挖掘)[13，38，41]**。尽管类似的困难实例挖掘可以应用于一级检测器，但由于存在大量容易得到的否定实例，因此效率低下[29]。

* 目标检测样本采样加权方法总结：

  1.RPN

  2.random sampling

  3.OHEM

  4.Focal Loss

  5.KL Loss

​           **以上方法都关注于不同的样本，导致其他样本不被重视。考虑到它们的确定，本文提出采样加权应依赖于数据与具体任务的。**

* **我们设计了一个样本加权网络来预测样本的分类和回归权重。网络将分类损失，回归损失，IoU和得分作为输入**，它可以将样本的当前上下文特征转换为样本权重

**Contribution：**

* **提出了一种用于物体检测的通用损失函数**，该函数涵盖了大多数基于区域的物体检测器及其采样策略，并在此基础上设计了一个统一的样本加权网络

  

##### 4.9 Noise-Aware Fully Webly Supervised Object Detection

**Abstract**

* 我们调查了新兴的学习目标检测器的任务，即在网络上仅具有图像级别的标签，而无需任何其他监督，例如精确注释或来自注释良好的基准数据集的其他图像。论文首次提出一种基于全网络监督的目标检测，即只使用网络爬取的图片来训练目标检测器。由于网络图像和标签带有很多噪声，文章首先把噪声分为两种：前景噪声和前景缺失。文章提出一种噪声分解框架和空间敏感的熵准则来减低前景噪声的负面影响，同时提出一种打包混合策略来减少前景缺失带来的负面影响。

**Introduction**

* **大多数对象检测方法[18、40、33、39、14、13、15]都依赖强监督，即来自具有良好注释的数据集[12、32]的真实边界框进行训练**

**Contribution**

* 我们提出了一种在端到端框架中结合了弱监督检测的残差学习结构，该结构可学习完全网络监督的检测器，并通过分解噪声和对干净数据进行建模来减少嘈杂标签的负面影响。
* 进一步提出了空间敏感的熵准则和bagging-mixup 学习，以分别显式估计背景标签有噪声的置信度，并分别抑制来自个不正确标签的前景噪声的影响。
* 我们的模型仅在网络数据上进行训练，每个类别约有4,000张图像，相对于流行基准（PASCAL VOC和MS COCO）上的最新方法而言，它们取得了重大改进。

---

##### 4.10 Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection

**Abstract**

* 通过减少训练过程中对强大监督的需求，弱监督学习已成为一种用于检测对象的引人注目的工具。但是其还有很多挑战。**1.** 对象实例的区分可能不明确**2.**探测器倾向于聚焦于区分部分而不是整个物体**3.**如果没有真实框，为了提高召回率对象预选框就必须是多余的，从而导致大量内存消耗。
* 为了解决这些问题，我们开发了一个**实例感知和针对上下文**的统一框架，它采用**实例感知**的**自训练算法**和可学习的**Concrete DropBlock**，同时设计了**具有存储效率的顺序批处理反向传播**
* 此外，该方法是第一个对基于ResNet的模型和弱监督视频对象检测进行基准测试的方法。

**Introduction**

* 关于物体检测的最新工作[18、36、35、27]取得了令人印象深刻的结果。但是，训练过程经常需要在精确的边界框方面进行严格的监督。**大规模获取此类注释可能会导致成本高昂，耗时甚至不可行。**
* 这激发了弱监督对象检测（WSOD）方法[5、46、23]，以较弱的监督形式（例如图像级类别标签）训练检测器。
* 弱监督对象检测的方法的缺陷
  * Instance Ambiguity：**1.Missing Instances**：背景中不太显眼的物体（姿势很少且比例较小）通常会被忽略**2.Grouped Instances**：在空间上相邻时，同一类别的多个实例被分组到单个边界框中。这两个问题都是由较大或较多的显着盒获得的分数高于较小或较少的显着盒所引起的。
  * Part Domination：**预测往往由对象的最有区别的部分主导，例如，在诸如动物和人的类上，模型通常会变成“面部检测器”，因为面部是最一致的外观信号**
  * **Memory Consumption**：现有的预选框生成方法[50，65]通常会生成密集的预选框。如果没有真实框的本地化，则必须维护大量预选边框才能实现合理的召回率，这需要很多内存。

**Contribution**

* 为了解决以上三个挑战，本文提出了一个统一的弱监督学习框架，该框架具有实例意识和以上下文为中心。
* 所提出的方法通过引入高级的自训练算法来解决实例模糊性，该算法中通过考虑更多实例相关的空间多样化约束来计算类别标签和回归目标形式的实例级伪真实边框。
* 通过引入称为“Concrete DropBlock”的参数空间缺失来解决“Part Domination”问题。该模块是端到端学习的，以对抗性地最大化检测目标，从而鼓励整个框架考虑上下文而不是专注于最有区别的部分.
* 最后，为了减轻内存消耗的问题，我们的方法采用了顺序批处理反向传播算法，该算法在最占用内存的阶段分批处理数据。
  这允许评估更大的深度模型，例如WSOD中的ResNet [19]，以及探索弱监督的视频对象检测（第4.3节）。

---

##### 4.11 CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection

**Abstract**

* 基于关键点的检测器具有良好的性能。然而，不正确的关键点匹配仍然普遍存在，严重影响了检测器的性能。在本文中，我们提出了 CentripetalNet，它利用向心偏移来对同一实例中的角点进行配对。CentripetalNet 预测角点（corner points）的位置和向心偏移（centripetal shift），并对偏移结果对齐（aligned）的角点进行匹配。**结合位置信息，我们的方法比传统的嵌入方法更精确地匹配角点**。corner pooling 将边界框内的信息提取到边界上。为了使角点处的信息更加敏感（更容易被察觉），我们设计了一个十字星可变形卷积网络（cross-star deforrossmable convolution network）来进行特征自适应。此外，我们通过为我们的CentripetalNet安装一个**mask prediction module（掩模预测模块）来探索anchor-free检测器上的实例分割**。

**Introduction**

* 目标检测是计算机视觉在自动驾驶、移动娱乐、视频监控等应用中的一个基本课题。其挑战来源于目标外观较大的尺度、形变、遮挡。随着深度学习的发展，目标检测取得了长足的进步。基于锚的方法在过去几年中引领了潮流，但是很难手动设计一组合适的锚。此外，基于锚的方法**在负锚和正锚之间存在显著的不平衡**。为了改进它，**CornerNet提出了一种将边界框表示为一对角点的新方法**，即左上角和右下角。基于这种思想，出现了许多基于角点的方法。corner-based 的检测框架逐渐引领了目标检测领域的**新趋势**。corner-based的检测框架分为**角点预测和角点匹配两个步骤**。在本文中，我们主要关注第二步。
* 常规方法主要采用**关联嵌入方法**对角点进行配对，其中网络需要学习每个角点的附加嵌入（即匹配角点），以识别两个角点是否属于同一个 边界框(bounding-box)。这样，如果两个角来自同一个box，它们将有一个相似的嵌入，否则，它们的嵌入将完全不同。基于关联嵌入的检测器在目标检测中取得了相当不错的性能，但是它们也有一些局限性。首先，在训练过程中**采用推拉损失（ push and pull loss）来学习每个点的嵌入**。Push loss（推损）将在不属于同一目标的点之间计算，以使它们彼此远离。而 pull loss（拉损）只在属于同一 物体的点之间进行考虑。因此，在训练过程中，**实际上训练了网络在对角线的所有潜在点内寻找唯一的匹配点**。**当一个训练样本中存在多个相似目标时，该方法对异常值非常敏感，训练难度将会急剧增加**。其次，嵌入预测**基于外观特征，不使用位置信息**，如果两个目标的外观相似，即使它们相距很远，网络也会为它们预测相似的嵌入。
* 基于上述考虑，为了更好地预测向心偏移（centripetal shift），我们提出了新颖的CentripetalNet，该算法采用了**基于向心偏移的角点匹配方法和一个十字星可变形卷积模块**（ cross-star deformable convolution module ）。给定一对角点，我们为每个角点定义一个2D向量，即**向心偏移**，其中，**向心偏移编码从角点到 box中心的空间位移**。这样，每个角点都可以根据向心偏移生成一个中心点，因此，**如果两个角点属于同一个边界框，则它们所生成的中心点应该是靠近的**。**匹配的质量可以由两个中心与该匹配的几何中心之间的距离来表示**。结合每个角点的位置信息，与联合嵌入方法相比，该**方法对异常点（值）具有较强的鲁棒性**。此外，我们还提出了一个新的组成部分，即 **cross-star** 可变形卷积（cross-star deformable convolution），不仅可以学习大的感受野接收场，而且**可以学习 cross-star 的几何结构**。我们观察到 corner-pooling 输出的特征图中有一些“十字星（cross stars）”。

**Motivation**

* **基于锚的方法在负锚和正锚之间存在显著的不平衡**

* ##### corner-based 的检测框架逐渐引领了目标检测领域的新趋势,常规方法主要采用关联嵌入方法对角点进行配对总结传统关联嵌入方法的局限性：

  - **只训练了网络在对角线的所有潜在点内寻找唯一的匹配点，无法处理多个相似目标的情况，对异常点非常敏感。**
  - **只基于外观特征，没有考虑位置信息。**

**Contribution**

* cross-star的边界包含目标的上下文信息，因为 corner-pooling 使用max和sum操作将目标的位置信息沿 “cross-star” 边界扩展到角。因此，我们将目标的几何和位置信息明确地嵌入到可变形卷积的偏移场中。与 CornerNet 相比，我们的模型配备了向心偏移和 cross-star 可变形卷积，在MS-COCO test-dev2017上实现了显着的性能提升，AP从42.1％提升到47.8％AP。此外，基于多任务学习在目标检测中的优势，我们首先增加了实例掩码分支（instance mask branch）来进一步提高检测精度。我们将RoIAlign应用于来自一组预测兴感趣区域(RoIs)的集合特征（ pool features），并将集合特征（ pool features）输入掩码头以生成最终的分割预测。
* CentripetalNet不仅以48.0％的AP胜过所有现有的无锚检测器，而且在实例分割方面也取得了与现有方法相当的性能。

**Related work**

* **Anchor-based approach**：基于Anchor的检测器在特征图的每个位置设置Anchor box。该网络预测每个Anchor box中有对象的概率，并调整锚定框的大小以匹配对象。通常，基于Anchor的方法可以分为两类，即两阶段方法和一阶段方法。从R-CNN系列方法[10，12，9]中导出两阶段方法，首先利用选择性搜索方法[32]提取RoI，然后对其进行分类和回归。Faster R-CNN[29]使用区域建议网络（RPN）通过修改预设Anchor boxes来生成RoI。Mask R-CNN[11]使用双线性插值用roalign层替换RoI Pool层。它的mask head使用自顶向下的方法来获取实例分割。在不提取RoI的情况下，一阶段方法直接对预先设置的Anchor boxes进行分类和回归。SSD[23]利用来自多个不同卷积层的特征映射，以不同的步幅对Anchor boxes进行分类和回归。与YOLO[26]相比，YOLOv2[27]使用预设Anchors。然而，上述方法存在着正负样本不平衡的问题。RetinaNet[20]使用焦点损失来缓解分类不平衡问题。RefineDet[37]通过引入锚定细化模块（anchor refifinement module）来过滤和消除负样本，用于细化FPN结构。

* **针对不同的问题，本文还提出了与基于锚的检测器协同工作，如改进锚的选择过程、改进特征学习过程、优化定位预测方法、改进损失函数。**

* **Anchor-free Approach**：对于基于Anchors的方法，Anchor boxes的形状应仔细设计以适合目标对象。与基于Anchors的方法相比，Anchor-free检测器不再需要预先设置Anchor boxes。主要提出了两种Anchor-free检测器。检测器的第一种类型**直接预测了目标的中心**。Yolov1[26]在目标中心点预测物体的大小和形状。DenseBox[14]引入了一种全卷积神经网络框架以获得高效率。UnitBox[36]使用IoU loss将四个边界作为一个整体进行回归。由于阳性样本的数量相对较少，这些检测器的召回率相当低。为了解决这个问题，FCOS[31]将对象边界框内的所有点作为正样本处理。它检测所有正点（positive points）以及从该点到边界框边界的距离。**对于第二种类型，检测器预测关键点并将其分组以获得边界框**。CornerNet[17]检测目标的左上角和右下角，并将它们嵌入到抽象的特征空间中。它通过计算每对点的嵌入之间的距离来匹配同一目标的角点。ExtremeNet[38]检测对象的顶部、左侧、底部、最右侧和中心点。结合Deep Extreme Cut[25]，可以使用极值点进行分割。这些检测器需要一些特定的分组方法来获得边界盒。RepPoints[35]使用可变形卷积网络（DCN）[6]获得用于表示目标的点集。转换函数经过精心设计，可将点集转换为边界框。CenterNet[7]在CornerNet中增加了一个中心检测分支，通过**中心点验证大大提高了性能**。

* 这些方法通常在错误检测较多的情况下达到较高的召回率。**主要的挑战在于如何匹配同一目标的关键点**。在这项工作中，**我们提出了一个向心偏移**，它将角点之间的关系编码，并通过预测的空间信息得到相应的中心，从而可以**通过它们的共享中心建立左上角和右下角之间的连接**。

### 五、总结

   最新目标检测的这些方法都是在以前的方法加以改进：

1.使用一些方法来提高检测的精确度和性能：**BiDet: An Efficient Binarized Object Detector**，**EfficientDet: Scalable and Efficient Object Detection**，**D2Det: Towards High Quality Object Detection and Instance Segmentation**

2.找到以往算法的设计缺陷就，然后提出改进方法，如**AugFPN: Improving Multi-scale Feature Learning for Object Detection**，对FPN中的设计缺陷加以改进。

3.特定场景的目标检测下提出的算法改进，例如定向和密集场景下的目标检测：**Dynamic Refinement Network for Oriented and Densely Packed Object Detection**，**Detection in Crowded Scenes: One Proposal, Multiple Predictions**

4.从目标检测的区域采样加权出发， 以往方法（1.RPN2.random sampling3.OHEM4.Focal Loss5.KL Loss都关注于不同的样本，导致其他样本不被重视，所以作者提出**Learning a Unified Sample Weighting Network for Object Detection**，样本的当前上下文特征转换为样本权重

5.现有的目标检测都依赖于精确注释或来自注释良好的基准数据集的强监督。所以提出了一些弱监督方法，例如：**Noise-Aware Fully Webly Supervised Object Detection**，**Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection**；有锚框的检测器，无锚框的检测器的区别，这里有一篇研究无锚和有锚检测器区别的paper：**Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection**；还有无锚的方法，从以往无锚的检测方法的一些缺陷出发加以改进：**CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection**

6.对抗的自适应检测方法：**Harmonizing Transferability and Discriminability for Adapting Object Detectors**

   对于目标检测这一任务而言，如果更关注性能指标，则可以参考two-stage系列的方法，而如果更关注实时性的要求，则可以关注one-stage的方法，这篇报告特别讲述了one-stage的YOLO算法，YOLO是以实时性为基础的快速目标检测算法，这种方法很快，也有许多工作，面向GPU和CPU实时性的目标检测算法都有人做出来，YOLO的含义也很有意思，YOLO——you only look once。而目标检测以后的发展方向，可以看出由于目标检测都依赖于精确注释或来自注释良好的基准数据集，很多都开始采用弱监督的学习方法，以更少的参数，更少的数据来获得更高的速度和精确度。

### 参考文献

[1] Krizhevsky A , Sutskever I , Hinton G . ImageNet Classification with Deep Convolutional Neural Networks[J]. Advances in neural information processing systems, 2012, 25(2).
[2] Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.
[3] Redmon J , Farhadi A . [IEEE 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - Honolulu, HI (2017.7.21-2017.7.26)] 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision & Pattern Recognition. IEEE, 2017:6517-6525.
[4] Redmon, J., and Farhadi, A.: 2016, arXiv e-prints, arXiv:1612.08242.
[5] Girshick R , Donahue J , Darrell T , et al. Region-Based Convolutional Networks for Accurate Object Detection and Segmentation[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2015, 38(1):142-158.
[6] Girshick R . Fast R-CNN[J]. Computer Science, 2015.
[7] Ren S , He K , Girshick R , et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks[J]. 2015.
[8] Liu W , Anguelov D , Erhan D , et al. SSD: Single Shot MultiBox Detector[J]. 2015.
[9] Lin T Y , Goyal P , Girshick R , et al. Focal Loss for Dense Object Detection[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017, PP(99):2999-3007.
[10] Everingham M , Eslami S M A , Van Gool L , et al. ThePascalVisual Object Classes Challenge: A Retrospective[J]. International Journal of Computer Vision, 2015, 111(1):98-136.
[11] Lin T Y , Maire M , Belongie S , et al. Microsoft COCO: Common Objects in Context[J]. 2014.
[12] http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture11.pdf
[13] https://blog.csdn.net/iamoldpan/article/details/78799857
[14] Ioffe S , Szegedy C . Batch normalization: accelerating deep network training by reducing internal covariate shift[C]// International Conference on International Conference on Machine Learning. JMLR.org, 2015.
[15] https://towardsdatascience.com/training-object-detection-yolov2-from-scratch-using-cyclic-learning-rates-b3364f7e4755
[16] http://ethereon.github.io/netscope/#/gist/d08a41711e48cf111e330827b1279c31	

[17] Wang Z, Wu Z, Lu J, et al. BiDet: An Efficient Binarized Object Detector[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 2049-2058.

[18]Guo C, Fan B, Zhang Q, et al. Augfpn: Improving multi-scale feature learning for object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 12595-12604.

[19]Zhang S, Chi C, Yao Y, et al. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 9759-9768.

[20]Dong Z, Li G, Liao Y, et al. Centripetalnet: Pursuing high-quality keypoint pairs for object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10519-10528.

[21]Cao J, Cholakkal H, Anwer R M, et al. D2Det: Towards High Quality Object Detection and Instance Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 11485-11494.

[22]Chu X, Zheng A, Zhang X, et al. Detection in Crowded Scenes: One Proposal, Multiple Predictions[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 12214-12223.

[23]Pan X, Ren Y, Sheng K, et al. Dynamic Refinement Network for Oriented and Densely Packed Object Detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 11207-11216.

[24]Chen C, Zheng Z, Ding X, et al. Harmonizing Transferability and Discriminability for Adapting Object Detectors[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 8869-8878.

[25]Guo J, Han K, Wang Y, et al. Hit-Detector: Hierarchical trinity architecture search for object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 11405-11414.